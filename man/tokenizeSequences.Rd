% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizeSequences.R
\name{tokenizeSequences}
\alias{tokenizeSequences}
\title{Tokenize Amino Acid Sequences}
\usage{
tokenizeSequences(
  tokenizer,
  aa_sequences,
  padding = TRUE,
  truncation = TRUE,
  return_tensors = "pt"
)
}
\arguments{
\item{tokenizer}{The tokenizer object returned by \code{\link{huggingModel}}.}

\item{aa_sequences}{A character vector of amino acid sequences (e.g., CDR3 sequences).}

\item{padding}{A logical or string. If TRUE, pads sequences to the length
of the longest sequence in the batch. Defaults to TRUE.}

\item{truncation}{A logical. If TRUE, truncates sequences to the model's
maximum input length. Defaults to TRUE.}

\item{return_tensors}{A string specifying the format for the returned tensors.
"pt" for PyTorch tensors, "tf" for TensorFlow. Defaults to "pt".}
}
\value{
The tokenized output, typically a dictionary-like object containing
  'input_ids' and 'attention_mask'.
}
\description{
Takes a vector of amino acid sequences and uses a Hugging Face
  tokenizer to convert them into numerical input IDs suitable for model input.
  The tokenizer should be obtained from \code{\link{huggingModel}}, which
  manages the Python environment via basilisk.
}
\examples{
sequences <- c("CASSLGTGELFF", "CASSIRSSYEQYF", "CASSYSTGELFF")
\donttest{
  # Initialize model and tokenizer
  hf_components <- huggingModel()

  # Tokenize CDR3 sequences
  tokenized <- tokenizeSequences(hf_components$tokenizer,
                                 sequences)

  # Pass tokenized output to proteinEmbeddings
  embeddings <- proteinEmbeddings(hf_components$model,
                                  tokenized,
                                  pool = "mean",
                                  chunk_size = 32)

  # Clean up
  basilisk::basiliskStop(hf_components$proc)
}
}
\seealso{
\code{\link{huggingModel}}, \code{\link{proteinEmbeddings}},
  \code{\link{runEmbeddings}}
}
