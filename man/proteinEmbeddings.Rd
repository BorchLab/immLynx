% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/proteinEmbeddings.R
\name{proteinEmbeddings}
\alias{proteinEmbeddings}
\title{Get Protein Embeddings from a Model}
\usage{
proteinEmbeddings(
  model,
  tokenized.batch,
  pool = c("none", "mean", "cls"),
  chunk_size = NULL,
  prefer_dtype = c("float16", "bfloat16", "float32"),
  prefer_device = c("auto", "cuda", "mps", "cpu")
)
}
\arguments{
\item{model}{HF model (from AutoModel or similar), typically obtained via
\code{\link{huggingModel}}.}

\item{tokenized.batch}{A *list* of tokenized tensors OR a list of such lists
(i.e., already minibatched). If you pass a single big batch, set chunk_size.
Typically obtained via \code{\link{tokenizeSequences}}.}

\item{pool}{One of "mean", "cls", or "none". "mean" is recommended for
sequence-level embeddings.}

\item{chunk_size}{If tokenized.batch is a single big batch, split it into
chunks of this many sequences. Ignored if you pre-batched upstream.}

\item{prefer_dtype}{One of "float16", "bfloat16", "float32". Lower precision
uses less memory but may reduce accuracy.}

\item{prefer_device}{One of "auto", "cuda", "mps", "cpu". "auto" will select
the best available device.}
}
\value{
An R matrix [n_sequences x hidden] if pool != "none".
        If pool == "none", returns a list of arrays per chunk.
}
\description{
Applies a pre-trained model to a batch of tokenized sequences
  to generate embeddings. This is the core embedding function used by
  \code{\link{runEmbeddings}}.
}
\examples{
sequences <- c("CASSLGTGELFF", "CASSIRSSYEQYF", "CASSYSTGELFF")
\dontrun{
  # Full workflow: load model, tokenize, embed
  hf_components <- huggingModel()
  tokenized <- tokenizeSequences(hf_components$tokenizer,
                                 sequences)

  # Mean pooling (recommended for sequence-level tasks)
  embeddings <- proteinEmbeddings(hf_components$model,
                                  tokenized,
                                  pool = "mean",
                                  chunk_size = 32)
  dim(embeddings)  # [n_sequences x hidden_dim]

  # CLS token embedding
  cls_emb <- proteinEmbeddings(hf_components$model,
                               tokenized,
                               pool = "cls",
                               chunk_size = 32)

  # Per-token embeddings (no pooling)
  token_emb <- proteinEmbeddings(hf_components$model,
                                 tokenized,
                                 pool = "none",
                                 chunk_size = 32)

  # Use GPU with half precision for speed
  embeddings_gpu <- proteinEmbeddings(
      hf_components$model, tokenized,
      pool = "mean", chunk_size = 64,
      prefer_device = "cuda",
      prefer_dtype = "float16")
}
}
\seealso{
\code{\link{runEmbeddings}} for a higher-level wrapper that works
  directly with Seurat/SingleCellExperiment objects.
}
